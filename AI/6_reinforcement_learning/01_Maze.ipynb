{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"RL_1_maze.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"WJYocnuxUtAH"},"source":["<div style=\"width: 100%; clear: both;\">\n","<div style=\"float: left; width: 50%;\">\n","<img src=\"http://www.uoc.edu/portal/_resources/common/imatges/marca_UOC/UOC_Masterbrand.jpg\", align=\"left\">\n","</div>\n","<div style=\"float: right; width: 50%;\">\n","<p style=\"margin: 0; padding-top: 22px; text-align:right;\">22.418 · Aprenentatge automàtic</p>\n","<p style=\"margin: 0; text-align:right;\">Grau en Ciència de Dades Aplicada</p>\n","<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudis de Informàtica, Multimèdia i Telecomunicació</p>\n","</div>\n","</div>\n","<div style=\"width:100%;\">&nbsp;</div>\n","\n","# Reinforcement learning 1: the maze\n","\n","\n","## Introduction \n","\n","This example shows a complete reinforcement learning (RL) example using \"classical\" Q-learning, with a table of states/actions that gets filled as the agent trains, and with all the RL aspects directly programmed here.\n","\n","The environment used is a rather simple one, a maze where the player has to find its way from the start (blue square) to the end (red square).\n","\n","![maze_step.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAoAAAAKZCAYAAAAoFJpwAAAABHNCSVQICAgIfAhkiAAAABl0RVh0U29mdHdhcmUAZ25vbWUtc2NyZWVuc2hvdO8Dvz4AACAASURBVHic7d15nN11fe/x95mZTEIyIZMQSAgQICwSJIR9R3ZBtC0qKkVt0fYhYm29+tDe3nq7Pqxae7X3trYWl9paW0VkURGQsG8lbIEEJAiEnQQCZCbbTJLJnPsHYQmQBSFMZj7P53+Tk9/vfH/n9/ud85rv75wzjfvvv7/ZHNaerUZ3BACAoatr+fL09/ambWH3kozrGJmFvT0DPSYAADax7p7etIzrGDnQ4wAA4E0yZosRaRnoQQAA8OYSgAAAxbQN9AAAAFi3lX09eaT7rixc+kiWrexKkowa3pmtR+2YyWPemva2LV7zOhv33ntv840eKAAAr9+TS+flrgVXp69/1ave3tYyLHtNPCYTOnZ+Tet1CRjgFfrS0/PqT7aDR39WLu/JYN+Kjdbfm54V5jMYWp5cOi93PHH5OuMvSfr6V+WOJ2bkyaUPvqZ1C0CAl+m974J85U+/nAvnDdZ86s2t3/pMPvVXF+SBvoEey5ug2Z1bvvMX+bOzr8vC/oEeDLwxVvb15K4FVyd57heb9rYRr/g/L/5bM3ctuDorV/du9Pq9BxA2qJnlj87MpRdfndvufSxdK9syeuIumX7ESXnn23bLlm/qr1H9mX/p3+YLFz6SzqM/nb84bfe0P3/T6gdy7p//n1w95r35q88dn/GNV25Hz6Mzc8nFV+e2ex/Pop5mRoydmMl7HJFT3n9UdhrxigXeBP15/KIv5m8uejpTP/QX+eQRY/PiKFbkVz/6Qv7+yp4c9odfzoff+iY9XfU9mhnnXJ2F27wzB0we9ubc54as7srcq36WS667Mw8+3ZPG6O0y7fjTcvpxUzLyVXdbf/pW9mVj58NW3np2PvPt27PqhQXaMvWDX8qn3rZlNnRUvJ5l3zCNLbPnQXuk9V9+kvNv3ScfO+hNvG/YRB7pvmutmb/fOfrz+dUTs3LdPRckSY7Y4zezx3YH5VuXfz5J0te/Mg8vmpPdxh+4UesXgLBezSy/97x89euX54n27bPvISfmiFErs+Dum3P9D/9v7nrw9/K5M/ZL55v1atN7T664+pH0NfvzzE0zcutJu+WwjbrzZpbfe36++k8z8kTrpEw76PgcPqYly599PPOe7Ulr+0C9XPana1FX+ps9+eUlv8h9B52W3dcUbXPhDfnZ9QvT32xNV1dvmul4U17Ue+++Itc+vkX2/djR2W4zeYbsf3Jmfnrp3LTve3R+623JIzddkZnnfTedU/4yp05pfZ1rb6a3pzfNlh1zzIdPyu7Dk6SR0ZO32IjH+/Us+0ZqZNS0k3L05Ftz0eXXZ/4BJ2eS61sMck8tfXitn2fNuzr773JcWhotSZrZfdL+ufOha9f6P08vfyS7RQDC67f6kcz44ZV5vG2PfOCPP5ljJqw5Zd5xXKb965fznZt/lIsO2jMfmvpkZnzjP3PtQ09l0bKVaYwcnyn7HJtT3n1Udh713Ethf9c9ufTcn+SGex7P4sbY7HLIb+X0d++fbdqS9D/83PIPP5VFS1ek2d6ZHfY+Pu8/7dhMeWGKp5muW6/ILV0jMvWAKXnstl/mqhvn55CTJ234vRyrH8mMH16Rx1vfkvf98Sdz3LYvm9lqLsmNX/98vjdvn3z8Sx/NPsOT5qKr8/f/+5w8fdRn8tentueq58e3ZEWyxTZ5yyEHZttnZ+e2uU9kSaMzUw55T37n1P0y/rX0SHNFurt7kuEj0v7sf+fSmSdmtyPHppEVuffyK3J/Y0RGNFZmcffi5wKwuSx3/OBr+eGtC7NkRTNbbL1bDnvPh3PK3p1ZdsM/5k++d3deesWzZdwx+fQXTsuuS9bz2K9lRe69bXaWjpqeg9468rmIeem+2Zht3wRjbJl0Qv7oC0dn+IjhaSTpm/Bs7vr6zMxf0JvmlFFpJFm14Oac94Of5eYHns3qjgkZs3r1Rr7Jp5me5cuTYRMz9cD9Mr1t7dueufGf86X/mJedP/Sn+cTh47J01nfzN9+8Jzud8ac58+Ax61n2VWzoOH/+9lc7j0YuzY3/tJ5j9P275sADd8xF59+eWQtOyiQFyCZy5plnvuq/n3322W/o/SxftXitn2+bd0VaWlqz785HJ0lmP3xdZt536drLrOze6PU7Q2A9+ufPzuwF/enY9/gcMeElr24tndnvhEOyTboy586H0tffncfueySLxh6YUz90ek45bEKeuemH+YdvXfvce5JWP57Lzv5GfnZvW6b/1odz2pFbZf6V3803L300/Uny/PKdB+S9H/xQ3nv4+Dxz84/znYvufzEW+ufnxmvuzcrOA/L200/KQVv157Ebrs39G/E2tee3Y9Tex+XIl8dfkjQ6ste+u6W9Z27m3L8yz80Y/jIP9W+Zt07bKW0vGd+pHzo975zazNwrL8pNS3bK0e95f07eo5EHrvqPnHf7so2+7JgkaS7NkiXNtEx6W06Y1pq5l1+Vh/qSZtetmXFTVyYfe3ymDmtm6ZI1621ska13PygnnvrhnHH6idll1dxc9m/n587eRraY+q6cedZZOeusj+e3D9s2bY32TNp/32zf3MBjv/YDlXkP96RlhynZ8fmH6bVu+yYZY0tGrIm/NJfnvjn3ZXnb5Ow+Zc1M26oH8tNv/FuufrQj+//maXnvsXtk3EaHeDOLu5akv/lIrv3BObnw8pszr+v5o66RrQ49PadOb8ld5/8wNz40K+eec2ua+38gv33Q2DTWu+yr2NBxvr7zqLmBYzSNjN1554xtLsiDD/W8tuMQNkMvn0VvNFoyasSWL/w8aviYNbOB61tq3cwAwnr0L12Spc1GxoztzMtfTxtjxqazpZkHlixN/5p34rVO3CtHHDY9bTkkOzf/On93xXWZOf/IvKP3xlz3UF92POXDOfWoCWlp7pLe+/885826M/NP3iHbrVln68RpOfLw6Wnr3z6L7/5yLps3L8/075YJLUnfA9fnhsf6s/Xxh2b3UTtl9METc9XFt+TaOb+Z3fYbud7Tvn/p0ixtNrLluM6XnPTN9Nx/TX5+++LsdNS7sv/0g7Pnj3+Z2bfemxV77pb7fnl/Vm25f/bb9cVgbJ04LUccPj2te/bll3f8KM/udHCOf9suaZm6InNmn5/HHnkqzQN3fslYmlmx+Jl09z6XMY3WkencqiMvrLG5LEuXN9PYZmIOP/Hw3PzVG3L5ncfnxCeuyNzWvXPGUTvn3uubWbZkafqTtKYl2x1w4prHq5kpvXNz13nz88Sz/dl30pTsPS7pe+Ly/L87nswWe52eM9/9lgx/6Nz1P/Yvff7s786i7maGTxn7ivfWbfy2b8IxNpfngUu+kW9fuyy7vvtjOXbiczesfvCW3PJUIzu/56M5/YSt08jytN93beY+sp6D4gWNdOxyaE5oLE7XUw/kpplX5dKfX5t3f/pTOXHysKQxNgef9v7M/uK/5gdfvSerOw7Kxz+wf8Y0NmLZdVjncf7C7a9+Hr1zA8doS+e4dLb0p7urO82M8j5ANok3eqZvXUYOG5PFK55+4ee3TT0lu2+7X2574Iokyf67HJe+/lW55u7zXlymfctXrGddBCCsR0vH6HQ0mlm8qCurs/1aU+bN7kXp6m9kZEdHWrLyZUu2ZdL2E9PaPy/PdvWnv+fZdDf788yFf5k/uPD5FTTTHLM4S17tU4uNjozuaElz+Yo1a16Re268Nc9kQk6YNj69S5dnzFv3yaRLL8mcG2Zl8b6HZ8x6t6MjHY1muhZ1pS+TX/jgSO9jt+eqK+fn0Gkn54AJe+eIfbfMnXfclDnPrs6cu3vSecD+2XVYkpdN6jTWPC7ze3vTTNIYNTodjWTRqlUvm3lZmTv/8y/znTuem6ZsGX98PvvX78suz9d0syfLljfTNnJURk85KkfvcnXOv+S7WdK9IOMO/WD2Gd2SR7doZHXP8qxoJsNWP5XbfnJufnH7Q1m4tC+trauzujk+/c8/hiseyM++c2Ee6Dg8nzjjiGzTmqxatIHHfq1foJtp9ictLetOhw1ue98mGmNzaeZe8I/5lxnPZqdT/ihnnrDDCyG9enF3ljbbsvs2Y3+N6GnJtgf8Rt5zwHM/rXr8knztSz/JpZffnaM/uk+GJ2l0Tsthe22Z229YnM49D8hbRjc2etn1esVx/nJrn0eN7dZzjCZJS0saSfr7zf8x+G3dseNaAbjj1lNzy/2XZdaDVydJ+purM23y4WsvM2rHjV6/AIT1aNl2WqZNuDiX3nFlbjhpjxz9/JvG+rty+4yb8lRzTA7de6e05VcvW7I/Ty98Jv2NjowZ3ZKWYWPS0WjJlif+YX73oDEvvki3jspWrUlWv/yeG2su9z33U3PZ7Nx4x+I0+7tz2Vc/l8te+j/n3pBbnj4sx49bz3ZMnJo9xl2cK+68Ojc+teeL27GWEdnz+KOzw8yLM+M/urOwZ+sccciuedV5nEbL2qHRaKwjPIZlt3d8LGcdumYGcPg2mfjS4Fq9PMtXJO3DhyeNrXLIsfvm4rNn5ldtu+V9x0zJsMb8DG9vJMuXpyfNNG85N/9++YPZ9TfOyOnTx2bFbd/PP1yyYs2D1JO5F/x7Ln9yYt7+mfdlr47nRtSy5QYe+7W2oyNbjm6kd/GSrMw6niDXu+3NLNkkY+zLE1ecnbNnPJOdTv0f+cRx27/46e8krZ1js2VjVZ54dEFWT9/+FbPVr8WwbXbItiOSR5ctT18zGd5oZvncC/OjmSuz4x475Mmbzsl5++6c0/d65QzbK5dNmn196W9tS+urHiBrH+evtPZ5tKFjtLm4O0v6G+nc8s35wBBsSpPHvDUPL5r9wieBv3fN36x1+6wHr34hBpOkraU9kzv32uj1C0BYn9Ydc8IHjsmd/3Rlzvnbv819B0/P9h0rsuCum3PrvMXp3P8jeddeI14IuFXzrs9FVy7JVqvm5borHk9ju3dlv+1a0to8JIdOuj6XXHtBLms9JLuOa8/qpYsyfOrJmbTBV6pmltw5M3f3tGaHY87Ie6d3vHBLzz0/zb9e9lBm3jI/x564nlW07ZK3v/egzPr2zJzzlb/LvMP2zY6dbVlyz9NrvQ+uZdsjc+I+V+Rbt96f1p1OyaE7vt6niJaM3WnvjF3Xlq3ozYpmI+3tw9JIIyOnHZe3T3sycyaclEO3aiRpz/D2Rvp7e9LbTEY0+9Ns9mfViuVZtqQtK1asfqEdeu/7af7rmqcy/C3vzHaL78kddySNlrHZaepreOxbts3k7YZl9WMP5bG+Q7L7r7P5m2KMPbNz8cX3ZeXEw7Nf51O56/ankiSNMTtl713GpXXnw3Lk5Ovzk198M9/qOybTtu7PfU+/4reKV7finlxy7i/Tst026WhdkQVzrsnNS0dkt+lvychG0lw+Nxd+/9osnfrBfOrju2X2P34xP/r+Odnz8x/Jvu1z17ts//wZ+eqXL8jCvX8vf/bR/TN6Ix/CdZ1HyfqO0WaWPvJIns64TNteADL4tbdtkb0mHpM7npiR9fyWtEYje008Ou2tr/yuwHURgLBejYya+r589nPb5eKfX5NZ/31pZq1szehtds6hp3407zrmLWt/Bcyqp3LnpefmqeWt2WrXY/OR00/KDq1JslPe9cmz0vbjn+aGK8/Pzb2NjBy3fQ7e9pgcMnkDL1bNRbl95tysHLFnjjv5gEzd8iXzTZN7ss+N38xtN9+cR4+ftt7tGLPf7+aPPzU5F116Q+Zc+7PcsiIZtsXoTJgyNTt2rpmWa3Rk2sFvzcjbZ2fXYw5be7ZuU1i5IiuSDBve/txj0LZj3v4H/ytvf/72Znva25N0PxeKow98T067/79y8XXfy9d/sTqtwzsydrup2WpEfx67cXYW9jfTf89F+fY9azZn+IE582u/v/GPfWNkpu6ze9rnzMmsee/N7ru/1u8B3DRjbD7zeJ7oaaZv2fX5z29e/8K9Ddv7I/nKHxySka3b58RPfCItP/5Jrrnmx7lzZWtGjBqfybtO2uD3VDaXL0vXgjm549aFWbyyNaPG75iD3ndG3n3kVmlkVR645Jxcv3hK3vOHh2artpa87QMn5qYv/jTn/vTQ7HHy+pZNmq3tGTG8PcOHD3ttnzhc53mUdR+jze7cOev+rB53RKZPfr1fjQObhwkdO2efSSes+VNw63ijREt79pp49Gv+U3D+FjC8Efpm57uf/efcsfdZ+epHpw++36yay/Pw7bMyf8WS3D3j4sxqPSGf+5PfyOueAByMeu/O9//q67lt+w/nLz5x2Jv3HY+bTF96l/Wm7xXP9I002oZn1IjNaCev7zzawDG66qEL8sWvXJEtTvmzfPbtE3zFBUPKytW9eXjRnDy9/NEsW9mVJBnV3pmtR03O5M69XtPM3/M2ozMfGDD9CzLr4nMzY0HSueuR+eiH3lEz/pJkxJ551ynTc+e/X5IZDxyY9+26mfw1kF9X3+x873+endte8XVBjQw/8Mx87ff3HRwvBOs7RptdueXn12bhhGPz6aPEH0NPe+uI7Db+wI3+kueNYQYQ4OWay/LEoz0ZP3n8Wh+2GJSaSzL//vlZ8irP9I3Rk7LrtkPj/XLNZY/nsZ4J2WH8oMhZGHACEACgGDPlAADFCEAAgGIEIABAMY1s+NsFAQAYQtqSZPz47w70OBggCxeeMdBDYIB0dydj1vcHhBnSGo1Gmk2//1fl/K+t0Wi4BAwAUI0ABAAoRgACABQjAAEAihGAAADFCEAAgGIEIABAMQIQAKAYAQgAUIwABAAoRgACABQjAAEAihGAAADFCEAAgGIEIABAMQIQAKAYAQgAUIwABAAoRgACABQjAAEAihGAAADFCEAAgGIEIABAMQIQAKAYAQgAUIwABAAoRgACABQjAAEAihGAAADFCEAAgGIEIABAMQIQAKAYAQgAUIwABAAoRgACABQjAAEAimkb6AHAm6XR+MZG/b9m86xNPBIAGFgCkCFtY6NvXcuIQQCGIgHIkPPrRN/GrEsMAjBUCECGjDcy/Na3fiEIwGDnQyAMCZs6/gbqvgBgUxCADHoDEWQiEIDBzCVgBq2BjjCXhAEYrMwAMigNdPy91OY0FgDYGAKQQWdzDK7NcUwAsC4CkEFlcw6tzXlsAPBSAhAAoBgByKAxGGbYBsMYAUAAMigMprAaTGMFoCYByGZvMAbVYBwzAHUIQACAYgQgm7XBPJM2mMcOwNAmAAEAihGAAADFCEA2W0PhEupQ2AYAhh4BCABQjAAEAChGALJZGkqXTofStgAwNAhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCCbpWbzrIEewhtmKG0LAEODAAQAKEYAAgAUIwDZbA2FS6dDYRsAGHoEIABAMQIQAKAYAchmbTBfQh3MYwdgaBOAAADFCEA2e4NxJm0wjhmAOgQgg8JgCqrBNFYAahKADBqDIawGwxgBQAACABQjABlUNucZts15bADwUgKQQWdzDK3NcUwAsC4CkEFpcwquzWksALAx2gZ6APDrej68Go1vDOj9A8BgYwaQQW8gQkz8ATCYCUCGhDczyMQfAIOdS8AMGZv6krDwA2CoEIAMOS8Ntdcbg6IPgKFIADKk/ToxKPoAGOoEIGUIOwB4jg+BAAAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBi2gZ6AAys7u6BHgEDpbOzka6u5kAPgwHUaDQGeggMIOd/bY0kzWbTQVBRo9GIfV+X/V+b/V9bd3cyZsxAj4KB0mg0XAIGAKhGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxbUnS3T3Qw2Cg2Pe12f+12f91dXY20tXVHOhhMIAaSZrNpoOgou7uZMyYgR4FA6XRaMS5X5f9X5v9X1uj0XAJGACgGgEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxbQlSaPRGOhxAAOgu3ugR8BAsv9rs/9rayRpNpvNgR4HA6C7OxkzZqBHwUBpNBpx7tdl/9dm/9fWaDRcAgYAqEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDECEACgGAEIAFCMAAQAKEYAAgAUIwABAIoRgAAAxQhAAIBiBCAAQDFtSdLdPdDDYCB0djbS1dUc6GEwgJz7tdn/tdn/tbUlyZgxAz0MBop9X1dXV9P+L8z+54Jdtx7oITCAXAIGAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBlJcfAgAAAZBJREFUCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxAhAAoBgBCABQjAAEAChGAAIAFCMAAQCKEYAAAMUIQACAYgQgAEAxjSTNgR4EAABvnv8PC7lJRLp1BPcAAAAASUVORK5CYII=)\n","\n","Source: [https://github.com/MattChanTK/gym-maze](https://github.com/MattChanTK/gym-maze)\n","\n","It is integrated into [OpenAI Gym](https://gym.openai.com/), which is a framework for RL with many different environments. It has become one of the main references for RL learning and research. One of its advantages is that all the environments, regardless of their specific features, share a common interface, that can be summarized in the following steps:\n","\n","1. Create the environment\n","2. Get the current state of the environment, or more precisely the observation of the environment, what the agent can perceive\n","3. The agent decides its next action and executes it on the environment\n","4. The agent gets a reward\n","5. Go back to point 2 until the end of the episode or other custom condition\n","\n","\n","## Setting up the environment\n","\n","As this Maze project is not included with OpenAI Gym, it has to be installed, which simply requires to extract the provided ```gym_maze.zip``` file to the current directory.\n","\n","Besides, some standard Python packages have to be installed using ```pip```.\n","\n","\n","Once installed Maze integrates into OpenAI Gym, so if the installation is successful you will be able to create the Maze environment and test your RL algorithms on it.\n","\n","***\n","\n","*Note*: Jupyter and PyGame do not interact perfectly, so it is possible that you will have to rerun some cells because the previous PyGame window has not closed completely."]},{"cell_type":"markdown","metadata":{"id":"-nnpW31pUtAJ"},"source":["## Exploring the environment\n","Before delving into RL itself, let us just try the Maze environment so that we get familiar with the Gym API, running the environment and rendering it. \n","\n","First, the desired environment is created, here a 5x5 maze with a fixed layout (other mazes are randomly generated on each episode). \n","\n","Second, the environment is reset to take it to the initial state, with the player at the blue square. \n","\n","Third, the environment is rendered (if RENDER==True) so we can see it on screen. As it uses [PyGame](https://www.pygame.org/), it will show in a new window. By default rendering is disabled, as it is not supported directly in Google Colab."]},{"cell_type":"code","metadata":{"id":"qlbHghxBVM5M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610031518694,"user_tz":-60,"elapsed":7343,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"0942a796-0fd7-4ca5-e401-4062baf3d27c"},"source":["!pip install numpy gym pygame\n","!unzip gym_maze.zip"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.19.4)\n","Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n","Collecting pygame\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/01/da/4ff439558641a26dd29b04c25947e6c0ace041f56b2aa2ef1134edab06b8/pygame-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (11.8MB)\n","\u001b[K     |████████████████████████████████| 11.8MB 5.6MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n","Installing collected packages: pygame\n","Successfully installed pygame-2.0.1\n","Archive:  gym_maze.zip\n","   creating: gym_maze/\n","  inflating: gym_maze/__init__.py    \n","   creating: gym_maze/envs/\n","  inflating: gym_maze/envs/__init__.py  \n","  inflating: gym_maze/envs/maze_env.py  \n","  inflating: gym_maze/envs/maze_generator.py  \n","   creating: gym_maze/envs/maze_samples/\n","  inflating: gym_maze/envs/maze_samples/maze2d_100x100.npy  \n","  inflating: gym_maze/envs/maze_samples/maze2d_10x10.npy  \n","  inflating: gym_maze/envs/maze_samples/maze2d_3x3.npy  \n","  inflating: gym_maze/envs/maze_samples/maze2d_5x5.npy  \n","  inflating: gym_maze/envs/maze_view_2d.py  \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":69},"id":"eZDJ5CHrUtAK","executionInfo":{"status":"ok","timestamp":1610031590028,"user_tz":-60,"elapsed":1749,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"764072f7-50f8-447d-b56f-f92203b90bfa"},"source":["import gym_maze\n","import gym\n","import pygame\n","\n","RENDER = False\n","\n","env = gym.make('maze-sample-5x5-v0', enable_render=RENDER)\n","\n","obs = env.reset()\n","\n","if RENDER:\n","  env.render()\n",";"],"execution_count":2,"outputs":[{"output_type":"stream","text":["pygame 2.0.1 (SDL 2.0.14, Python 3.6.9)\n","Hello from the pygame community. https://www.pygame.org/contribute.html\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["''"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"eEeUkYXgUtAL"},"source":["*Note*: the semicolon (;) at the end of the previous cell prevents Jupyter from printing the result of ```env.render()```, which is an array with all the cells of the maze.\n","\n","Before we run the game, let us check some very important elements: the **observation space** and the **action space**.\n","\n","The **observation space** is what the agent receives from the environment on each iteration, and its format can be easily obtained in Gym:"]},{"cell_type":"code","metadata":{"id":"f6awShrVUtAL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610031632034,"user_tz":-60,"elapsed":953,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"c2bdf6e8-2a2a-492f-8cf1-164e95fcc59d"},"source":["env.observation_space"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 4, (2,), int64)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"smj2O0JiUtAM"},"source":["Which means that the observation space is a 2D box (like an array) with indices from 0 to 4 on both axes. This gives a total of 5*5 = 25 states, a tiny number compared to most RL environments, and the main reason why we begin with this example. \n","\n","If we check the result from the ```env.reset()``` instruction we can check the specific value for the observation after resetting the environment:"]},{"cell_type":"code","metadata":{"id":"p46ci1hxUtAM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610031677194,"user_tz":-60,"elapsed":794,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"f73cf14a-a115-4347-d05e-e10d96f69c4f"},"source":["obs"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0.])"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"6bkKbugbUtAM"},"source":["Which means that the current position of the agent is 0, 0, i.e. the top-left corner. This is the only information needed to define the state of this environment, as nothing else can change here. \n","\n","On the other hand, the **action space** is the set of actions the agent can choose from, and it can be checked with:"]},{"cell_type":"code","metadata":{"id":"e794jh9zUtAN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610031694707,"user_tz":-60,"elapsed":774,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"701f5a15-a1c4-4f7a-ec94-f8f46af3c0d7"},"source":["env.action_space"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(4)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"bWVOTWCGUtAN"},"source":["Which means that the action space is *discrete* (actions are on/off, differently from *continous* actions such as the torque of a motor, that can be a number inside a range), and there are four actions, that correspond with the agent moving in one of the four directions. \n","\n","The ```step()``` method executes one step of the RL loop. It receives the action to be executed by the agent, then returns four values, consequence of the action chosen: the observation of the new state of the environment, the reward obtained, a boolean ```done``` that is ```True``` when the episode finishes (for example the agent reaches the goal) and a fourth value with debugging information (that should not be used in competitions).\n","\n","Here we can see an example of ```step()``` and the results obtained:"]},{"cell_type":"code","metadata":{"id":"xZpFQb6FUtAO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610031782067,"user_tz":-60,"elapsed":737,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"c9a467fe-9fb7-4924-f772-8cb31e912cc8"},"source":["obs, rwd, done, info = env.step(2)\n","\n","if RENDER:\n","  _ = env.render()\n","\n","print(obs)\n","print(rwd)\n","print(done)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[1 0]\n","-0.004\n","False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Hz6Y61bWUtAO"},"source":["Where the agent has chosen action 2 (move right), so environment has changed its state.\n","\n","Exploring the four possible actions you can find out their meaning: 0=up, 1=down, 2=right, 3=left. This encoding depends on the specific environment, and you should check its documentation or play with it to find them out. However, for RL the exact encoding of actions is not really important, as we are not going to use them explicitly.\n","\n","You can also see the new observation (new position of the agent), the reward obtained (there is a penalty for each step, the only positive reward comes from reaching the goal), and finally ```done==False``` which means that the episode has not ended yet.\n","\n","Finally, for clean operation the environment must be closed:"]},{"cell_type":"code","metadata":{"id":"ToE6qHGiUtAP","executionInfo":{"status":"ok","timestamp":1610031908594,"user_tz":-60,"elapsed":745,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["env.close()\n","pygame.quit()"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uludF7gAUtAP"},"source":["## First run\n","\n","Before we add RL to this environment, let us do a very simple run, with randomly picked actions for 200 steps, so we can see the environment in action and most of what will be the RL loop.\n","\n","Method ```sample()``` picks a random value from the action space."]},{"cell_type":"code","metadata":{"id":"MozMRiyKUtAP","executionInfo":{"status":"ok","timestamp":1610031948180,"user_tz":-60,"elapsed":854,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["import gym \n","import gym_maze\n","\n","env = gym.make('maze-sample-5x5-v0', enable_render = RENDER)\n","\n","env.reset()\n","\n","for step in range(200):\n","    if RENDER:\n","      env.render()\n","    env.step(env.action_space.sample())\n","env.close()\n","pygame.quit()"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-0tfAbYuUtAP"},"source":["As you have seen, the agent moves around randomly (and most likely it will not reach the goal).\n","\n","**Note**: you may have to restart Jupyter's kernel because sometimes the PyGame window does not close properly."]},{"cell_type":"markdown","metadata":{"id":"ncQ2paY5UtAQ"},"source":["# RL with Q-learning\n","Now that you know how to use the environment, it is time to use Q-learning so that the agent learns how to get to the goal in this maze. The basic Q-learning includes a table of states x actions (so 25 x 4), where it will slowly store the estimated reward of choosing action *a* in state *s*. With some probability *epsilon* the agent will choose a random action instead of the best one, in order to encourage exploration.\n","\n","First we will define the required functions and data for Q-learning, then the RL loop itself in the Maze environment.\n","\n","## Supporting functions\n","\n","### Create the Q-table\n","Given an environment, the Q-table must be of size states x actions. ```env.observation_space.high``` gives the highest indices of the observation space, and with that we can determine the number of states. The ```n``` attribute is the number of actions in the action space.\n","\n","Initially all the expected rewards are zero.\n","\n","*Note*: ```+ 1``` is needed as this is Python, so if the last index is 4 then there are 5 elements."]},{"cell_type":"code","metadata":{"id":"cmouS-sIUtAQ","executionInfo":{"status":"ok","timestamp":1610032115309,"user_tz":-60,"elapsed":702,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["import numpy\n","\n","def create_q_table(env):\n","    rows, cols = env.observation_space.high + 1\n","    actions    = env.action_space.n\n","    \n","    return numpy.zeros((rows * cols, actions))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmr8o_BnUtAR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610032120037,"user_tz":-60,"elapsed":850,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"55a4d786-11bd-45a9-f3e3-984a5a16a742"},"source":["q_table = create_q_table(env)\n","q_table.shape"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(25, 4)"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"7n_9q0TuUtAR"},"source":["### Observations to Q-table\n","When we translate multidimensional value to a single index in order to fit it in a matrix, such as it happens here from the x,y position of the agent to a row number in the Q-table, we usually talk about these positions in the 1D index as **buckets**. Here we have to translate the x, y position of the agent to a row number in the Q-table, so our function will be called ```obs_to_bucket``` for that reason. \n","\n","*Note*: ```env.observation_space.high == [4, 4]```, as they are the highest possible values for the row/column indices. In turn, that means that there are 5x5 cells, that is why the ```+ 1``` is needed to correctly translate from row/column to a single index."]},{"cell_type":"code","metadata":{"id":"Dg-0gH3-UtAS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1610032373887,"user_tz":-60,"elapsed":674,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"4c7b6d92-2c8d-4cb6-e903-9ad6b42b32f5"},"source":["def obs_to_bucket(env, obs):\n","    row, col = obs\n","    return int(row*(env.observation_space.high[1] + 1) + col)\n","\n","# Example calls\n","print(obs_to_bucket(env, [0, 2]))\n","print(obs_to_bucket(env, [2, 0]))\n","print(obs_to_bucket(env, [2, 2]))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["2\n","10\n","12\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"B1youh97UtAS"},"source":["### Select action\n","Q-learning chooses a random action with probability epsilon or the best action (highest value in the Q-table) otherwise. "]},{"cell_type":"code","metadata":{"id":"Ury-6bMAUtAS","executionInfo":{"status":"ok","timestamp":1610032400742,"user_tz":-60,"elapsed":730,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["import random\n","\n","def choose_action(env, q_table, state, epsilon):\n","    if random.random() < epsilon:\n","        return env.action_space.sample()\n","    else:\n","        return int(numpy.argmax(q_table[state]))"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zJUIbB3_UtAT"},"source":["### Update Q-table\n","After an action is executed, the agent receives the new state (observation) and a reward. It is time to update the Q-table, considering which was the previous state, the action chosen, the reward obtained and the new state. This function performs this update, considering also the learning rate and the discount factor (future rewards are not as worthy as immediate ones). "]},{"cell_type":"code","metadata":{"id":"zYOrdNtvUtAT","executionInfo":{"status":"ok","timestamp":1610032456500,"user_tz":-60,"elapsed":957,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["def update_q_table(q_table, prev_state, new_state, action, reward, learning_rate, discount):\n","    # The best expected reward in the new state\n","    best_exp_rwd = numpy.amax(q_table[new_state])\n","    \n","    # Expected reward of the previous state and action\n","    prev_exp_rwd = q_table[(prev_state, action)]\n","    \n","    q_table[prev_state, action] += learning_rate * (reward + discount*best_exp_rwd - prev_exp_rwd)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZcFATFajUtAT"},"source":["## The RL loop\n","After setting up all the required auxiliary functions, the actual training of the agent using RL can start. After the initialization of the environment and other data structures, the training is a loop with the following steps:\n","\n","1. Get the observation from the environment (to infer the current state)\n","2. Choose the best action for the current state given the expected rewards in the Q-table *or* choose a random action with probability *epsilon*\n","3. Execute the action\n","4. Retrieve the new observation and the reward for the action just executed\n","5. Update the Q-table accordingly, considering: the previous state, the action chosen, the reward obtained and the new state the action has taken the environment to\n","\n","The next function executes the RL loop for a number of episodes (games)."]},{"cell_type":"code","metadata":{"id":"KkFjZzKIUtAU","executionInfo":{"status":"ok","timestamp":1610032571606,"user_tz":-60,"elapsed":958,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["def rl_loop(env, num_episodes, max_steps, learning_rate, epsilon, discount):\n","    \n","    q_table = create_q_table(env)\n","    \n","    episode_total_rewards = []\n","    \n","    for episode in range(num_episodes):\n","        \n","        # We want a fresh maze on each new episode\n","        # env.reset() returns the observation of the initial state\n","        obs = env.reset()\n","        \n","        # Convert observation to state\n","        curr_state = obs_to_bucket(env, obs)\n","        \n","        # Reward of the current episode\n","        episode_reward = 0.0\n","        \n","        for step in range(max_steps):\n","            \n","            # Choose action and execute it\n","            action = choose_action(env, q_table, curr_state, epsilon)\n","            \n","            obs, reward, done, info = env.step(action)\n","            \n","            # Move on to current state\n","            prev_state = curr_state\n","            curr_state = obs_to_bucket(env, obs)\n","            \n","            episode_reward += reward\n","            \n","            update_q_table(q_table, prev_state, curr_state, action, reward, learning_rate, discount)\n","            \n","            # Render the environment so we can see the result\n","            if RENDER:\n","              env.render()\n","            \n","            # Check if the episode ends (goal reached)\n","            if done:\n","                break\n","                \n","        # When the episode ends, do some updates\n","        episode_total_rewards.append(episode_reward)\n","        \n","        epsilon = max(0.1, epsilon*0.95)\n","    \n","    return q_table, episode_total_rewards"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hra2v6RqUtAU"},"source":["Finally it is time to start the environment and run the RL training algorithm, which is very simple as you can see here. The most important thing when calling ```rl_loop``` are the hyperparameters of Q-learning:\n","- ```num_episodes```: number of episodes the RL agent is going to be executed. Here each episode lasts until either the agent reaches the goal, or it reaches amaximum number of steps (see next parameter)\n","- ```max_steps```: if the agent does not reach the goal in this number of steps or less, the episode fails, basically meaning that the agent does not get the only positive reward, given when it reaches the goal. It is important to avoid the agent running randomly forever. The exact value depends on the complexity of the task, more complex tasks require extra steps for the agent to have a chance of finding the goal, at least at the beginning of the training\n","- ```learning_rate```: this value controls how the current reward modifies the existing expected reward in the q-table. A value of 1 would simply forget the previous values, while a value of 0 would never update the q-table\n","- ```epsilon```: the chance of randomly choosing an action instead of choosing the action with maximum expected reward. This parameter influeces the exploration vs exploitation aspect.  *epsilon* is usually high during the first episodes (exploration), and gradually decreases as the agent learns (explotation of acquired knowledge)\n","- ```discount```: future rewards are not as worthy as immediate ones. The discount factor models this, as it multiply future rewards, actually decreasing their importance. It is usually a value between 0.95 and 0.99, and the exact value depends on how distant in the future the rewards are expected to be\n"]},{"cell_type":"code","metadata":{"id":"l8ag1WDMUtAV","executionInfo":{"status":"ok","timestamp":1610032671852,"user_tz":-60,"elapsed":886,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}}},"source":["import gym \n","import gym_maze\n","\n","env = gym.make('maze-sample-5x5-v0', enable_render=RENDER)\n","\n","q_table, rewards = rl_loop(env, num_episodes=100, max_steps=50, learning_rate=0.8, epsilon=0.8, discount=0.99)\n","\n","env.close()\n","pygame.quit()"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FmUF5YIzUtAV"},"source":["# Analysis of the results\n","\n","You have seen on the game window how the agent learns the path to the goal more and more efficiently as it trains. Let us have a look at some of the internal results of the RL process in order to better understand what has happened there.\n","\n","## Q-table\n","The q_table is like the memory of the agent, it tells what is the expected reward at state *i* (row) if it takes action *j* (column).\n","For example, at state 0, which corresponds to cell 0,0, the best reward is for going right, so this will be the default action unless *epsilon* makes it choose a random action. At state 23, right above the goal, the best action is going south (a full 1.0 reward). The goal itself is state 24, where the episode ends and consequently no rewards are recorded."]},{"cell_type":"code","metadata":{"id":"-KTMRTF9UtAV","colab":{"base_uri":"https://localhost:8080/","height":824},"executionInfo":{"status":"ok","timestamp":1610032751082,"user_tz":-60,"elapsed":830,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"c3c11a62-0ff8-46d0-8a14-1f1768fe3c50"},"source":["import pandas as pd\n","df = pd.DataFrame(q_table)\n","df.columns = ['UP', 'DOWN', 'RIGHT', 'LEFT']\n","df"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>UP</th>\n","      <th>DOWN</th>\n","      <th>RIGHT</th>\n","      <th>LEFT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.781761</td>\n","      <td>0.814863</td>\n","      <td>0.828529</td>\n","      <td>0.814708</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.042280</td>\n","      <td>-0.044796</td>\n","      <td>-0.041186</td>\n","      <td>-0.042487</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.044088</td>\n","      <td>-0.044804</td>\n","      <td>-0.047115</td>\n","      <td>-0.044591</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-0.025894</td>\n","      <td>-0.026410</td>\n","      <td>-0.029672</td>\n","      <td>-0.025616</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.027945</td>\n","      <td>-0.029421</td>\n","      <td>-0.028975</td>\n","      <td>-0.027895</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.116555</td>\n","      <td>-0.047307</td>\n","      <td>0.840939</td>\n","      <td>0.781747</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>0.653764</td>\n","      <td>-0.045573</td>\n","      <td>-0.047318</td>\n","      <td>-0.047354</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>-0.045040</td>\n","      <td>-0.045888</td>\n","      <td>-0.046671</td>\n","      <td>-0.046569</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>-0.027340</td>\n","      <td>-0.028839</td>\n","      <td>-0.027875</td>\n","      <td>-0.028706</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>-0.029514</td>\n","      <td>-0.028496</td>\n","      <td>-0.027474</td>\n","      <td>-0.029322</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>0.833894</td>\n","      <td>0.853474</td>\n","      <td>-0.039729</td>\n","      <td>-0.039141</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>0.805682</td>\n","      <td>0.866135</td>\n","      <td>0.675400</td>\n","      <td>0.817820</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>0.843181</td>\n","      <td>0.878924</td>\n","      <td>0.865848</td>\n","      <td>0.686232</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>0.802335</td>\n","      <td>0.891843</td>\n","      <td>0.697143</td>\n","      <td>0.842606</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>0.878633</td>\n","      <td>0.708350</td>\n","      <td>0.904891</td>\n","      <td>-0.025856</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>-0.012647</td>\n","      <td>-0.012647</td>\n","      <td>-0.012929</td>\n","      <td>-0.013272</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>0.906630</td>\n","      <td>0.931192</td>\n","      <td>0.958419</td>\n","      <td>0.753109</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>0.944834</td>\n","      <td>0.731079</td>\n","      <td>0.741629</td>\n","      <td>0.920152</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>0.931386</td>\n","      <td>-0.018598</td>\n","      <td>0.880589</td>\n","      <td>0.880547</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>0.918072</td>\n","      <td>0.867808</td>\n","      <td>0.897477</td>\n","      <td>0.836532</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>-0.012647</td>\n","      <td>0.919692</td>\n","      <td>-0.011057</td>\n","      <td>-0.012246</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>0.601082</td>\n","      <td>0.972140</td>\n","      <td>0.919777</td>\n","      <td>0.906207</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>0.958230</td>\n","      <td>0.986000</td>\n","      <td>0.933126</td>\n","      <td>0.494930</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>0.777712</td>\n","      <td>1.000000</td>\n","      <td>0.946560</td>\n","      <td>0.946560</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["          UP      DOWN     RIGHT      LEFT\n","0   0.781761  0.814863  0.828529  0.814708\n","1  -0.042280 -0.044796 -0.041186 -0.042487\n","2  -0.044088 -0.044804 -0.047115 -0.044591\n","3  -0.025894 -0.026410 -0.029672 -0.025616\n","4  -0.027945 -0.029421 -0.028975 -0.027895\n","5   0.116555 -0.047307  0.840939  0.781747\n","6   0.653764 -0.045573 -0.047318 -0.047354\n","7  -0.045040 -0.045888 -0.046671 -0.046569\n","8  -0.027340 -0.028839 -0.027875 -0.028706\n","9  -0.029514 -0.028496 -0.027474 -0.029322\n","10  0.833894  0.853474 -0.039729 -0.039141\n","11  0.805682  0.866135  0.675400  0.817820\n","12  0.843181  0.878924  0.865848  0.686232\n","13  0.802335  0.891843  0.697143  0.842606\n","14  0.878633  0.708350  0.904891 -0.025856\n","15 -0.012647 -0.012647 -0.012929 -0.013272\n","16  0.906630  0.931192  0.958419  0.753109\n","17  0.944834  0.731079  0.741629  0.920152\n","18  0.931386 -0.018598  0.880589  0.880547\n","19  0.918072  0.867808  0.897477  0.836532\n","20 -0.012647  0.919692 -0.011057 -0.012246\n","21  0.601082  0.972140  0.919777  0.906207\n","22  0.958230  0.986000  0.933126  0.494930\n","23  0.777712  1.000000  0.946560  0.946560\n","24  0.000000  0.000000  0.000000  0.000000"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"markdown","metadata":{"id":"heAVbroIUtAV"},"source":["## Rewards \n","\n","The most important tool to understand the learning process in RL is to monitor the reward obtained on each episode, where we would expect to see a rising curve that gradually stabilizes. Let us verify that."]},{"cell_type":"code","metadata":{"id":"Lqn9h4uYUtAW","colab":{"base_uri":"https://localhost:8080/","height":285},"executionInfo":{"status":"ok","timestamp":1610032772005,"user_tz":-60,"elapsed":917,"user":{"displayName":"Carles Ventura","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhImvnSdfq8GVCeFZMwR3T0QjMyuAesBI1pc5H5Ng=s64","userId":"16735323099153148275"}},"outputId":"d5642ac1-0d6d-4987-8761-f7b0239880c6"},"source":["import matplotlib.pyplot as plt\n","\n","plt.plot(rewards)"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f6b6e9f3ac8>]"]},"metadata":{"tags":[]},"execution_count":17},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"_39WBQzkUtAW"},"source":["As you can see here, in the first episodes the agent does not reach the goal, so it gets a negative reward (it only gets positive rewards for reaching the goal). In some episodes, it reaches the goal but with a lot of steps, so the reward is around 0.8. From episode 25 on, it systematically reaches the goal in a very efficient way, but for one episode where it fails to reach it, most likely due to a chain of wrong random actions chosen.\n","\n","The conclusion here is that the RL loop needed around 40 episodes to train a proficient agent. Further training does not lead to significant improvements."]},{"cell_type":"markdown","metadata":{"id":"pwq74g3okaGl"},"source":["## Activities\n","\n","The previous example has shown how to create and train a Q-learning agent, using a Q-table to store the expected rewards for each (state, action) pair. Here we propose some questions or experiments to get more insight into the Q-learning algorithm.\n","\n","### Questions\n","*   The state of the environment is given simply by the position of the agent. How would it change if there was a cell with a hole (fixed position), so that if the agent falls in it dies and gets a negative reward? How much would the number of possible states grow? What else would you have to change?\n","*   Now imagine that there is an enemy wandering around. How does this affect the states of the game? \n","\n","### Experiments\n","*   Add one or more \"holes\" to the maze, so that if the agent falls in it dies and gets negative reward. You can check for the holes inside the RL loop, and give rewards/end episode accordingly.\n","*   Study the influence of the *epsilon* parameter. First, plot the values of *epsilon* as it decays. Then try different decay curves (change decay factor or minimum *epsilon*) and analyze the results. Can you improve the learning process this way?\n"]},{"cell_type":"code","metadata":{"id":"VUwBn9mwgP2E"},"source":[""],"execution_count":null,"outputs":[]}]}
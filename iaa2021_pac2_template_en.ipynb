{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "iaa2021_pac2_template_en.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "-kRePYp-EGzP",
        "ZHUoHxYBB6jo"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/moisesortega93/MCME-AI/blob/main/iaa2021_pac2_template_en.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pT7oHQaNFe5c"
      },
      "source": [
        "##### M0.508 · IAA · CET2 · 2020-21 · 2\n",
        "##### Màster Enginyeria Informàtica · Estudis d’Informàtica Multimèdia i Telecomunicació\n",
        "##### Universitat Oberta de Catalunya"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrG4AzzxUmC3"
      },
      "source": [
        "# CET2: FEATURE EXTRACTION AND CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6CG9OsqVOJ9"
      },
      "source": [
        "## INTRODUCTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMqpqloqVXmw"
      },
      "source": [
        "In this evaluation test we will study how to apply feature extraction and classification techniques in cartographic data about forest cover types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zHV-LgDVjm6"
      },
      "source": [
        "## SKILLS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHTnAQYlVqjK"
      },
      "source": [
        "In this assignment, the following general skills developed in the Master are addressed:\n",
        "* Ability to project, envision and design products, processes and\n",
        "facilities in all areas of computer engineering.\n",
        "* Abilities in mathematical modelling, calculation and simulation in\n",
        "technology centres and business engineering, particularly in research,\n",
        "development and innovation tasks in all areas related to computer\n",
        "engineering.\n",
        "* Ability to apply the knowledge acquired and solve problems in new or\n",
        "unfamiliar environments within broader and multidisciplinary contexts,\n",
        "and being able to integrate this knowledge.\n",
        "* Skills for continuous, self-directed and autonomous learning.\n",
        "* Ability to model, design, define architecture, implement, manage,\n",
        "operate, manage and maintain applications, networks, systems,\n",
        "services and computer content.\n",
        "\n",
        "The specific skills of this course that are addressed in this test are:\n",
        "* Understanding what machine learning is in the context of artificial\n",
        "intelligence.\n",
        "* Distinguishing between different types and methods of learning.\n",
        "* Applying the studied techniques to a real case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kRePYp-EGzP"
      },
      "source": [
        "## RESOURCES\n",
        "\n",
        "This CET requires the following resources:\n",
        "\n",
        "Provided files:\n",
        "\n",
        "  * iaa2021_pac2_template_en.ipynb\n",
        "\n",
        "Complementary: \n",
        "  * Course materials, library documentation (_scikit-learn_, _pandas_, _seaborn_,...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHUoHxYBB6jo"
      },
      "source": [
        "## SUBMISSION AND ASSESSMENT CRITERIA\n",
        "\n",
        "The CET must be submitted by **27th April 2021**. \n",
        "\n",
        "The final submission must be an edited version of this notebook (.ipynb). The use of Google Colab platform is encouraged (https://colab.research.google.com/). The source code solutions to the exercises must be implemented and run in the corresponding code cells and the related discussion and justified answers must be added to the corresponding text cell.\n",
        "\n",
        "All answers must be discussed and justified. **Answers without\n",
        "discussion will not be evaluated**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6A6AVoYWloS"
      },
      "source": [
        "## CET DESCRIPTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIPMykjfXB0q"
      },
      "source": [
        "In this assignment, data classification and dimensionality reduction\n",
        "techniques will be used on real world cartographic data about forest cover types.\n",
        "\n",
        "We will work with the data from a study published in:\n",
        "\n",
        "> Blackard, Jock A., and Denis J. Dean. \"Comparative accuracies of artificial neural networks and discriminant analysis in predicting forest cover types from cartographic variables.\" _Computers and electronics in agriculture_ 24.3 (1999): 131-151.\n",
        "\n",
        "The data set includes samples of seven forest cover types extracted from cover type maps created by the US Forest Service that were derived from aerial photographies. The independent cartographic variables were obtained from the US Geological Survey and the US Forest Service. The variables, among others, include elevation, horizontal distance to nearest surface water and relative measures of incident sunlight. Overall, the data set includes 581,012 instances and 54 variables.\n",
        "\n",
        "A full description of the dataset can be found in:\n",
        "\n",
        "https://archive.ics.uci.edu/ml/datasets/Covertype\n",
        "\n",
        "The goal of this assignment is to use and get familiar with different\n",
        "dimensionality reduction techniques and to carry out a comparative study of\n",
        "different data classification algorithms and validation techniques.\n",
        "\n",
        "The solutions to the exercises will be based on the open source library _scikit-learn_ for Python, which includes a wide selection of machine learning\n",
        "algorithms as well as preprocessing, validation and visualization techniques.\n",
        "Students are encouraged to refer to scikit-learn documentation available\n",
        "online:\n",
        "\n",
        "https://scikit-learn.org/stable/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IPa7a1yVr_Lm"
      },
      "source": [
        "## EXERCISE 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dPC37jIRgOF"
      },
      "source": [
        "First things first. Let's import some necessary packages and load the data. The original data set is available in _scikit-learn_ as part of the _datasets_ module.\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_covtype.html\n",
        "\n",
        "In this exercise we are going to work with a simplified version of the original dataset. The simplified version includes fewer samples and attributes. Study and run the code provided in the following cell and get familiar with the data set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCUe8cYdZPvX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f1564da-e02c-40bb-dc56-d64dcee8acc5"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "features, labels = datasets.fetch_covtype(return_X_y=True)\n",
        "\n",
        "n_samples = 5000\n",
        "n_attributes = 10\n",
        "df_features = pd.DataFrame(features[:n_samples, :n_attributes])\n",
        "df_labels = pd.DataFrame({'label': labels[:n_samples]})\n",
        "\n",
        "n_classes = len(np.unique(df_labels))\n",
        "\n",
        "print(\"No. of attributes = \" + str(len(df_features.columns)))\n",
        "print(\"No. of classes = \" + str(n_classes))\n",
        "print(\"No. of samples = \" + str(len(df_features)))\n",
        "for cl in np.unique(df_labels):\n",
        "  print(\"\\--No. of samples class \" + str(cl) + \" = \" \n",
        "        + str(df_labels[df_labels==cl].count()['label']))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No. of attributes = 10\n",
            "No. of classes = 7\n",
            "No. of samples = 5000\n",
            "\\--No. of samples class 1 = 557\n",
            "\\--No. of samples class 2 = 948\n",
            "\\--No. of samples class 3 = 643\n",
            "\\--No. of samples class 4 = 1249\n",
            "\\--No. of samples class 5 = 945\n",
            "\\--No. of samples class 6 = 479\n",
            "\\--No. of samples class 7 = 179\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lfukvlhnLxun"
      },
      "source": [
        "**1.a) (1 POINT) Explore the dataset features.**\n",
        "\n",
        "**Use the method _describe_ in _pandas.DataFrame_. What kind of information does it generate?**\n",
        "\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.describe.html\n",
        "\n",
        "**Analise the standard deviation of all the components. Do you think they span over similar ranges? Identify the attributes with maximum and minimum dispersion (standard deviation) and provide their values. Can you briefly describe (theoretically) what would happen if we used those features directly to perform a PCA analysis?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jcds1TxiniSZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "95b76672-453b-40ba-f1cd-73197a2c9c35"
      },
      "source": [
        "# Exercise 1.a: add and run your source code\n",
        "print(df_features.head())\n",
        "print(df_labels.head())\n",
        "\n",
        "df_features.info()\n",
        "df_features.describe()\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        0      1     2      3      4       5      6      7      8       9\n",
            "0  2596.0   51.0   3.0  258.0    0.0   510.0  221.0  232.0  148.0  6279.0\n",
            "1  2590.0   56.0   2.0  212.0   -6.0   390.0  220.0  235.0  151.0  6225.0\n",
            "2  2804.0  139.0   9.0  268.0   65.0  3180.0  234.0  238.0  135.0  6121.0\n",
            "3  2785.0  155.0  18.0  242.0  118.0  3090.0  238.0  238.0  122.0  6211.0\n",
            "4  2595.0   45.0   2.0  153.0   -1.0   391.0  220.0  234.0  150.0  6172.0\n",
            "   label\n",
            "0      5\n",
            "1      5\n",
            "2      2\n",
            "3      2\n",
            "4      5\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 5000 entries, 0 to 4999\n",
            "Data columns (total 10 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   0       5000 non-null   float64\n",
            " 1   1       5000 non-null   float64\n",
            " 2   2       5000 non-null   float64\n",
            " 3   3       5000 non-null   float64\n",
            " 4   4       5000 non-null   float64\n",
            " 5   5       5000 non-null   float64\n",
            " 6   6       5000 non-null   float64\n",
            " 7   7       5000 non-null   float64\n",
            " 8   8       5000 non-null   float64\n",
            " 9   9       5000 non-null   float64\n",
            "dtypes: float64(10)\n",
            "memory usage: 390.8 KB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "      <td>5000.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>2590.297600</td>\n",
              "      <td>155.529600</td>\n",
              "      <td>17.599000</td>\n",
              "      <td>183.294000</td>\n",
              "      <td>46.290200</td>\n",
              "      <td>1723.505600</td>\n",
              "      <td>212.261000</td>\n",
              "      <td>215.535000</td>\n",
              "      <td>130.761600</td>\n",
              "      <td>1517.390800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>389.357969</td>\n",
              "      <td>109.718326</td>\n",
              "      <td>9.400014</td>\n",
              "      <td>160.188788</td>\n",
              "      <td>56.633346</td>\n",
              "      <td>1564.713429</td>\n",
              "      <td>33.583068</td>\n",
              "      <td>25.150835</td>\n",
              "      <td>50.682969</td>\n",
              "      <td>1324.742471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1863.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-134.000000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>99.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>30.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>2220.000000</td>\n",
              "      <td>66.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>42.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>630.000000</td>\n",
              "      <td>195.000000</td>\n",
              "      <td>202.000000</td>\n",
              "      <td>97.000000</td>\n",
              "      <td>601.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>2677.500000</td>\n",
              "      <td>121.500000</td>\n",
              "      <td>17.000000</td>\n",
              "      <td>150.000000</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>1167.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>221.000000</td>\n",
              "      <td>135.000000</td>\n",
              "      <td>1124.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>2905.250000</td>\n",
              "      <td>263.250000</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>277.000000</td>\n",
              "      <td>74.000000</td>\n",
              "      <td>2207.000000</td>\n",
              "      <td>237.000000</td>\n",
              "      <td>233.000000</td>\n",
              "      <td>167.000000</td>\n",
              "      <td>2001.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3442.000000</td>\n",
              "      <td>360.000000</td>\n",
              "      <td>52.000000</td>\n",
              "      <td>997.000000</td>\n",
              "      <td>554.000000</td>\n",
              "      <td>6890.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>254.000000</td>\n",
              "      <td>248.000000</td>\n",
              "      <td>6853.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 0            1  ...            8            9\n",
              "count  5000.000000  5000.000000  ...  5000.000000  5000.000000\n",
              "mean   2590.297600   155.529600  ...   130.761600  1517.390800\n",
              "std     389.357969   109.718326  ...    50.682969  1324.742471\n",
              "min    1863.000000     0.000000  ...     0.000000    30.000000\n",
              "25%    2220.000000    66.000000  ...    97.000000   601.000000\n",
              "50%    2677.500000   121.500000  ...   135.000000  1124.000000\n",
              "75%    2905.250000   263.250000  ...   167.000000  2001.000000\n",
              "max    3442.000000   360.000000  ...   248.000000  6853.000000\n",
              "\n",
              "[8 rows x 10 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a_VnUxxSXQw"
      },
      "source": [
        "Exercise 1.a: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AYxcb3yzICQ"
      },
      "source": [
        "**1.b) (1 POINT) PCA analysis must always be performed on scaled data. The next exercises will allow us to study and learn about PCA and to explore the impact of data scaling in the process.**\n",
        "\n",
        "**Perform a PCA analysis to the data twice:**\n",
        "\n",
        "* **PCA analysis to the raw features.**\n",
        "* **PCA analysis to the standardised features using _StandardScaler_ data:**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
        "\n",
        "**Similarly to Exercise 1.a, in both of the PCA spaces, identify the attributes with maximum and minimum dispersion (standard deviation) and their corresponding values. Discuss the results.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBvx89fJu9nn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "b83d9cca-c48c-4f74-94be-a916db2c4d78"
      },
      "source": [
        "# Exercise 1.b: add and run your source code\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(df_features)\n",
        "X_transformed = pca.transform(df_features)\n",
        "\n",
        "# display the projected dataset\n",
        "\n",
        "for i, target_name in zip([0, 1], df_labels):\n",
        "    plt.scatter(X_transformed[df_labels == i, 0], X_transformed[df_labels == i, 1], label=target_name)\n",
        "plt.legend()\n",
        "\n",
        "df_features_scaled = StandardScaler(df_features)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-bcc2bef928e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_transformed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_labels\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: too many indices for array: array is 2-dimensional, but 3 were indexed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRqs5eZm6C-_"
      },
      "source": [
        "Exercise 1.b: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4amNjG605Tm_"
      },
      "source": [
        "**1.c) (1 POINT) Create three scatter plots of the first two components in:**\n",
        "\n",
        "* **The original raw features (original data set).**\n",
        "* **The PCA space obtained without data scaling (features from the original data set transformed to a the PCA space).**\n",
        "* **The PCA space obtained with data scaling (features from the original space, scaled and, then, transformed to a the PCA space).**\n",
        "\n",
        "**Discuss the obtained plots.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BROIyq1lIn8"
      },
      "source": [
        "# Exercise 1.c: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A0YrjB4X6Kqo"
      },
      "source": [
        "Exercise 1.c: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmvsaJ_v6wDR"
      },
      "source": [
        "**1.d) (1 POINT) For the PCA space obtained with data scaling and without data scaling, use _Seaborn_ library and its method _boxplot_ to create 10 box plots (each) to graphically depict the attributes (y axis) for each of the data set classes (x axis).**\n",
        "\n",
        "**Seaborn is a Python data visualization library based on _matplotlib_. It provides a high-level interface for drawing attractive and informative statistical graphics.**\n",
        "\n",
        "https://seaborn.pydata.org/\n",
        "\n",
        "https://seaborn.pydata.org/generated/seaborn.boxplot.html\n",
        "\n",
        "**Hint: Use matplotlib.pyplot.subplots with parameter _sharey=True_ to create a subplot grid of 2x10 axes (2 rows for the 2 PCA spaces and 10 columns for the 10 attributes).**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsKwrD16D2Ja"
      },
      "source": [
        "# Exercise 1.d: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-FkvWPlC_8y"
      },
      "source": [
        "Exercise 1.d: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtPT5c117I5_"
      },
      "source": [
        "**1.e) (1 POINT) Plot the cumulative explained variance ratio as a function of the number of components for the two PCA spaces (without and with previous scaling). Compare both plots and discuss the relation to the box plots in the previous exercise. In the case of scaled data, identify how many PCA components are necessary to represent 95% of the variance of the original data. What is the problem when PCA analysis is performed on unscaled data?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_os8DwPs5Qg"
      },
      "source": [
        "# Exercise 1.e: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdvvOqYrGDCf"
      },
      "source": [
        "Exercise 1.e: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8eC9MMj7zWh"
      },
      "source": [
        "**1.f) (1 POINT) Using the scaled features, rebuild the dataset for the 5 PCA components (_inverse_transform_ method) and calculate the loss of information with\n",
        "respect to the original set. To do so, use the average of the squared differences between each element of the reconstructed set and the original one. What is the relationship between this value and the\n",
        "cumulative variances plotted in the previous exercise?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfQm3WvE6Phf"
      },
      "source": [
        "# Exercise 1.f: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnbZg5wEGI5W"
      },
      "source": [
        "Exercise 1.f: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gua8EbpNGdMm"
      },
      "source": [
        "## EXERCISE 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLNe3s3YL3ed"
      },
      "source": [
        "In this exercise we are going to explore different classification algorithms, validation techniques and performance metrics. For that purpose, a new version of the data set is used. The number of samples is decreased even further and the labels are reduced to two classes to facilitate the comparison between the different classification algorithms. Study and run the code provided in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TgUJYgHCuEe4"
      },
      "source": [
        "n_samples2 = 700\n",
        "n_attributes2 = 54\n",
        "df_features2 = pd.DataFrame(features[:n_samples2, :n_attributes2])\n",
        "\n",
        "labels2 = [1 if value==2 else 0 for value in labels]\n",
        "df_labels2 = pd.DataFrame({'label': labels2[:n_samples2]})\n",
        "\n",
        "print(\"No. of attributes = \" + str(len(df_features2.columns)))\n",
        "print(\"No. of classes = \" + str(len(np.unique(labels2))))\n",
        "print(\"No. of samples = \" + str(len(df_features2)))\n",
        "print(\"\\--No. of samples class 0 = \" + \n",
        "      str(df_labels2[df_labels2==0].count()['label']))\n",
        "print(\"\\--No. of samples class 1 = \" + \n",
        "      str(df_labels2[df_labels2==1].count()['label']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Abeh-MpHL5cs"
      },
      "source": [
        "**2.a) (1.5 POINTS) Train the following classifiers, implemented in _scikit-learn_, by using 80% of the available data (training set) and report the training time for each of them (_timeit_ python module):**\n",
        "\n",
        "- **k Nearest Neighbors: 5 neighbours (first parameter).**\n",
        "\n",
        "- **Linear SVM: kernel=”rbf”, C=25, and the default values for the remaining parameters.**\n",
        "\n",
        "- **Decision Tree: criterion='entropy' max_depth=5, and the default values for the remaining parameters.**\n",
        "\n",
        "- **AdaBoost: default parameters.**\n",
        "\n",
        "- **Gaussian Naive Bayes: default parameters.**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YTa06DrcBbsg"
      },
      "source": [
        "# Exercise 2.a: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iRLrH6-hL-U8"
      },
      "source": [
        "Exercise 2.a: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vLMFICDMPG_"
      },
      "source": [
        "**2.b) (1 POINT) Testing the performance of the classifiers trained in Exercise 2.a.**\n",
        "\n",
        "**Plot in a single figure/axis all the ROC curves for the trained classifiers by using the remaining samples (20% test set). Discuss the results and compare the performance of the different algorithms. Which one would be your choice for the given task?**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.plot_roc_curve.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwXCXWDVM-oI"
      },
      "source": [
        "# Exercise 2.b: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aH2HDWN7MSBd"
      },
      "source": [
        "Exercise 2.b: Double-click (or enter) to add your answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6iwRT9EMG_F"
      },
      "source": [
        "**2.c) (1.5 POINTS) Train and validate the same classifiers described in Exercise 2.a by using-cross-validation with k=5 on the whole reduced dataset.Report accuracy, precision, recall, F1-score and ROC-AUC for each classifier and discuss the results.**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html\n",
        "\n",
        "**Hint: Use sklearn.model_selection.RepeatedStratifiedKFold (with parameters *n_repeats*=1 and _random_state_=1) as the _cv_ parameter of _cross_validate_ to create identical splits to validate all classifiers.**\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RepeatedStratifiedKFold.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTDXAV_e5P8W"
      },
      "source": [
        "# Exercise 2.c: add and run your source code"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSZCf7lTMKWH"
      },
      "source": [
        "Exercise 2.c: Double-click (or enter) to add your answer."
      ]
    }
  ]
}